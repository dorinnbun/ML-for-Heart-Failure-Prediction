{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "554786fa-d16c-4b34-b999-6d5ce611c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# --- Final Project: Heart Disease Prediction (Full Pipeline) ---\n",
    "# -------------------------------------------------------------------\n",
    "# This script covers the entire machine learning workflow:\n",
    "# 1. Setup and Data Loading\n",
    "# 2. Exploratory Data Analysis (EDA) with Visualization\n",
    "# 3. ETL and Feature Engineering\n",
    "# 4. Model Training and Hyperparameter Tuning\n",
    "# 5. Final Model Evaluation and Interpretation\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# 1. SETUP AND DATA LOADING\n",
    "# -------------------------------------------------------------------\n",
    "# Import necessary libraries for data manipulation, visualization, and modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5b989de-c3ee-424e-bf73-65164a9e978a",
   "metadata": {},
   "outputs": [],
   "source": [
    " df = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37f0f2f6-cdd5-4416-a4a8-133333c289c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
      "0   40   M           ATA        140          289          0     Normal    172   \n",
      "1   49   F           NAP        160          180          0     Normal    156   \n",
      "2   37   M           ATA        130          283          0         ST     98   \n",
      "3   48   F           ASY        138          214          0     Normal    108   \n",
      "4   54   M           NAP        150          195          0     Normal    122   \n",
      "\n",
      "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
      "0              N      0.0       Up             0  \n",
      "1              N      1.0     Flat             1  \n",
      "2              N      0.0       Up             0  \n",
      "3              Y      1.5     Flat             1  \n",
      "4              N      0.0       Up             0  \n"
     ]
    }
   ],
   "source": [
    "df = print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ab0b865-b77c-42ed-940e-dbf6236b8820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# -------------------------------------------------------------------\n",
    "# This cell will only run if the DataFrame was loaded successfully.\n",
    "if df is not None:\n",
    "    print(\"\\n\\n---\\n| 2. EXPLORATORY DATA ANALYSIS (EDA)\\n---\\n\")\n",
    "\n",
    "    # --- Basic Information ---\n",
    "    print(\"\\n--- Dataset Info ---\")\n",
    "    df.info()\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # --- Descriptive Statistics for Numerical Features ---\n",
    "    print(\"\\n--- Descriptive Statistics (Numerical Features) ---\")\n",
    "    print(df.describe())\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\"\"\n",
    "    Interpretation:\n",
    "    - Age: The patient ages range from 28 to 77, with an average of 53.5.\n",
    "    - RestingBP & Cholesterol: Both have minimum values of 0, which are physiologically impossible.\n",
    "      These are likely data errors and will be handled in the ETL phase.\n",
    "    - MaxHR: Maximum heart rate achieved varies widely from 60 to 202.\n",
    "    \"\"\")\n",
    "\n",
    "    # --- Distribution of Categorical Features ---\n",
    "    print(\"\\n--- Distribution of Categorical Features ---\")\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_features:\n",
    "        print(f\"\\nValue counts for {col}:\")\n",
    "        print(df[col].value_counts())\n",
    "    print(\"-\" * 40)\n",
    "    print(\"\"\"\n",
    "    Interpretation:\n",
    "    - Sex: The dataset is predominantly male (78.9%).\n",
    "    - ChestPainType: 'ASY' (Asymptomatic) is the most common type, which is a key indicator.\n",
    "    - RestingECG: 'Normal' is the most frequent result.\n",
    "    - ExerciseAngina: More patients do not have exercise-induced angina ('N').\n",
    "    - ST_Slope: 'Flat' and 'Up' are the most common ST slope patterns.\n",
    "    \"\"\")\n",
    "\n",
    "    # --- Target Variable Distribution ---\n",
    "    print(\"\\n--- Target Variable (HeartDisease) Distribution ---\")\n",
    "    print(df['HeartDisease'].value_counts())\n",
    "    sns.countplot(x='HeartDisease', data=df)\n",
    "    plt.title('Distribution of Heart Disease')\n",
    "    plt.show()\n",
    "    print(\"\"\"\n",
    "    Interpretation:\n",
    "    The dataset is fairly balanced, with slightly more patients having heart disease (508)\n",
    "    than not (410). This means we don't need to use complex techniques like SMOTE for\n",
    "    class imbalance.\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    # --- Visualization of Numerical Feature Distributions ---\n",
    "    print(\"\\n--- Visualizing Numerical Feature Distributions ---\")\n",
    "    numerical_features = df.select_dtypes(include=np.number).columns.drop('HeartDisease')\n",
    "    df[numerical_features].hist(bins=20, figsize=(15, 10), layout=(2, 3))\n",
    "    plt.suptitle('Distribution of Numerical Features')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    print(\"\"\"\n",
    "    Interpretation:\n",
    "    - Age and MaxHR appear to be somewhat normally distributed.\n",
    "    - RestingBP and Cholesterol are skewed by the '0' values, which we will clean.\n",
    "    - Oldpeak is heavily skewed to the right, indicating most values are low.\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    # --- Correlation Heatmap ---\n",
    "    print(\"\\n--- Correlation Matrix of Numerical Features ---\")\n",
    "    # We compute the correlation matrix only on numerical columns\n",
    "    numeric_df = df.select_dtypes(include=np.number)\n",
    "    corr_matrix = numeric_df.corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title('Correlation Heatmap of Numerical Features')\n",
    "    plt.show()\n",
    "    print(\"\"\"\n",
    "    Interpretation:\n",
    "    - MaxHR has a moderate negative correlation with HeartDisease (-0.40). Lower maximum heart\n",
    "      rate is associated with a higher chance of heart disease.\n",
    "    - Oldpeak has a moderate positive correlation with HeartDisease (0.40). Higher ST depression\n",
    "      is associated with a higher chance of heart disease.\n",
    "    - Age also shows a positive correlation (0.28).\n",
    "    - There are no extremely high correlations between independent features, reducing concerns\n",
    "      about multicollinearity.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1768ff9c-2ce1-4320-a05a-ea10d1bf9be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ETL & FEATURE ENGINEERING\n",
    "# -------------------------------------------------------------------\n",
    "if df is not None:\n",
    "    print(\"\\n\\n---\\n| 3. ETL & FEATURE ENGINEERING\\n---\\n\")\n",
    "\n",
    "    # --- Data Cleaning (TRANSFORM step of ETL) ---\n",
    "    print(\"\\n--- Cleaning Data ---\")\n",
    "    # Drop duplicates\n",
    "    initial_rows = df.shape[0]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(f\"Removed {initial_rows - df.shape[0]} duplicate rows.\")\n",
    "\n",
    "    # Correct the physiologically impossible '0' values in RestingBP and Cholesterol\n",
    "    # by replacing them with the median of the respective column.\n",
    "    for col in ['RestingBP', 'Cholesterol']:\n",
    "        median_val = df[col].median()\n",
    "        num_zeros = (df[col] == 0).sum()\n",
    "        if num_zeros > 0:\n",
    "            df[col] = df[col].replace(0, median_val)\n",
    "            print(f\"Replaced {num_zeros} zero values in '{col}' with the median value ({median_val}).\")\n",
    "    print(\"âœ… Data cleaning complete.\")\n",
    "\n",
    "    # --- Feature Engineering Pipeline ---\n",
    "    # Separate features (X) and target (y)\n",
    "    X = df.drop('HeartDisease', axis=1)\n",
    "    y = df['HeartDisease']\n",
    "\n",
    "    # Re-identify numerical and categorical features after cleaning\n",
    "    numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Create preprocessing pipelines for numerical and categorical data\n",
    "    # 1. Numerical features will be scaled to have a mean of 0 and std deviation of 1.\n",
    "    # 2. Categorical features will be converted into numerical format using one-hot encoding.\n",
    "    numerical_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "\n",
    "    # Bundle preprocessing steps into a single ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    print(\"\\nâœ… Preprocessing and feature engineering pipeline created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "772296e3-5085-44e6-90dd-8c4993dc734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. MODEL TRAINING & HYPERPARAMETER TUNING\n",
    "# -------------------------------------------------------------------\n",
    "if df is not None:\n",
    "    print(\"\\n\\n---\\n| 4. MODEL TRAINING & HYPERPARAMETER TUNING\\n---\\n\")\n",
    "\n",
    "    # --- Train-Test Split ---\n",
    "    # Split data into 80% for training and 20% for testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(f\"Data split into {X_train.shape[0]} training samples and {X_test.shape[0]} testing samples.\")\n",
    "\n",
    "    # --- Model Pipeline and Hyperparameter Tuning ---\n",
    "    # Define the model to be used: RandomForestClassifier\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Create the full pipeline: Preprocessor -> Classifier\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', rf)\n",
    "    ])\n",
    "\n",
    "    # Define the grid of hyperparameters to search through\n",
    "    # Using a smaller, more focused grid for demonstration purposes\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [10, 20, None],\n",
    "        'classifier__min_samples_split': [2, 5],\n",
    "        'classifier__min_samples_leaf': [1, 2]\n",
    "    }\n",
    "\n",
    "    # Set up GridSearchCV to find the best parameters using 5-fold cross-validation\n",
    "    # We optimize for 'recall' as it's crucial to identify as many true cases as possible\n",
    "    print(\"\\nðŸš€ Starting hyperparameter tuning with GridSearchCV...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        model_pipeline,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='recall',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Fit the grid search to the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model found by the grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    print(\"\\nâœ… Hyperparameter tuning complete.\")\n",
    "    print(f\"Best Recall Score from CV: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Best Parameters Found: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2ddefe7-213c-4391-8b77-2bfc116f45b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. FINAL MODEL EVALUATION & INTERPRETATION\n",
    "# -------------------------------------------------------------------\n",
    "if df is not None:\n",
    "    print(\"\\n\\n---\\n| 5. FINAL MODEL EVALUATION & INTERPRETATION\\n---\\n\")\n",
    "    print(\"\\nðŸ“Š Evaluating the best model on the unseen test set...\")\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # --- Calculate and Display Metrics ---\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    print(\"\\n--- Final Evaluation Metrics ---\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}  <-- Key metric for medical diagnosis\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(f\"AUC-ROC:   {roc_auc:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- Classification Report ---\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Heart Disease', 'Heart Disease']))\n",
    "    print(\"-\" * 29)\n",
    "\n",
    "    # --- Confusion Matrix ---\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted No Disease', 'Predicted Disease'],\n",
    "                yticklabels=['Actual No Disease', 'Actual Disease'])\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\"\"\n",
    "    --- Final Interpretation ---\n",
    "\n",
    "    The final Random Forest model performs very well on the unseen test data.\n",
    "\n",
    "    - Key Metric (Recall): The model achieved a recall of 0.91 for the 'Heart Disease' class.\n",
    "      This is excellent for a medical context, as it means the model correctly identified\n",
    "      91% of all patients who actually have heart disease. Minimizing \"false negatives\"\n",
    "      (failing to detect the disease) is the top priority.\n",
    "\n",
    "    - Precision: The precision of 0.88 means that when the model predicts a patient has\n",
    "      heart disease, it is correct 88% of the time.\n",
    "\n",
    "    - Accuracy & F1-Score: The overall accuracy of 88.5% and an F1-score of 0.90 indicate\n",
    "      a strong, balanced model.\n",
    "\n",
    "    - Confusion Matrix Breakdown:\n",
    "      - True Negatives (Top-Left): 70 patients correctly identified as not having heart disease.\n",
    "      - False Positives (Top-Right): 12 healthy patients were incorrectly flagged. This is an\n",
    "        acceptable trade-off for higher recall.\n",
    "      - False Negatives (Bottom-Left): 9 patients with heart disease were missed. This is the\n",
    "        most critical error, and our model keeps this number low.\n",
    "      - True Positives (Bottom-Right): 93 patients correctly identified as having heart disease.\n",
    "\n",
    "    Conclusion: This model demonstrates strong potential as a clinical decision-support tool.\n",
    "    It is highly effective at its primary goal of detecting patients with heart disease,\n",
    "    while maintaining a good balance of overall accuracy.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf40de-feda-4fae-adda-b2a9a6c28949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
